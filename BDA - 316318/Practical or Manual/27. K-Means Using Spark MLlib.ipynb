{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  K-Means Clustering Using Spark MLlib\n",
        "\n",
        "This guide demonstrates how to apply **K-Means clustering** using **Spark MLlib** on a structured dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‚ Sample Dataset (`customers.csv`)\n",
        "\n",
        "```text\n",
        "customer_id,age,annual_income,spending_score\n",
        "1,19,15,39\n",
        "2,21,15,81\n",
        "3,20,16,6\n",
        "4,23,16,77\n",
        "5,31,17,40\n",
        "6,22,17,76\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Step 1: Load Dataset into Spark\n",
        "\n",
        "```scala\n",
        "val df = spark.read\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .csv(\"customers.csv\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Step 2: Prepare Data for Clustering\n",
        "\n",
        "Select numerical features and convert to vector:\n",
        "\n",
        "```scala\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "\n",
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"age\", \"annual_income\", \"spending_score\"))\n",
        "  .setOutputCol(\"features\")\n",
        "\n",
        "val featureDF = assembler.transform(df).select(\"customer_id\", \"features\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Step 3: Apply K-Means Algorithm\n",
        "\n",
        "```scala\n",
        "import org.apache.spark.ml.clustering.KMeans\n",
        "\n",
        "val kmeans = new KMeans()\n",
        "  .setK(3)\n",
        "  .setSeed(1L)\n",
        "  .setFeaturesCol(\"features\")\n",
        "  .setPredictionCol(\"cluster\")\n",
        "\n",
        "val model = kmeans.fit(featureDF)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Step 4: Get Cluster Assignments\n",
        "\n",
        "```scala\n",
        "val clusteredDF = model.transform(featureDF)\n",
        "clusteredDF.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Step 5: Save Clustering Results\n",
        "\n",
        "```scala\n",
        "clusteredDF.write.mode(\"overwrite\").csv(\"output/kmeans_result\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Œ Sample Output\n",
        "\n",
        "```text\n",
        "+-----------+-------------+-------+\n",
        "|customer_id|features     |cluster|\n",
        "+-----------+-------------+-------+\n",
        "|1          |[19,15,39]   |1      |\n",
        "|2          |[21,15,81]   |0      |\n",
        "|3          |[20,16,6]    |2      |\n",
        "|4          |[23,16,77]   |0      |\n",
        "|5          |[31,17,40]   |1      |\n",
        "|6          |[22,17,76]   |0      |\n",
        "+-----------+-------------+-------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "i4fIX5U_E2G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark"
      ],
      "metadata": {
        "id": "-nVvBAS-lSCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# K-Means Clustering Using Spark MLlib\n",
        "# With Sample Random Data\n",
        "# ---------------------------------------------\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import rand\n",
        "import random\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1. Create Spark Session\n",
        "# ---------------------------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KMeansExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. Generate Sample Random Data\n",
        "# ---------------------------------------------\n",
        "# Create random 2D points around 3 centers\n",
        "\n",
        "data = []\n",
        "\n",
        "# Cluster 1 around (2, 2)\n",
        "for _ in range(100):\n",
        "    data.append((random.gauss(2, 0.5), random.gauss(2, 0.5)))\n",
        "\n",
        "# Cluster 2 around (8, 8)\n",
        "for _ in range(100):\n",
        "    data.append((random.gauss(8, 0.5), random.gauss(8, 0.5)))\n",
        "\n",
        "# Cluster 3 around (5, 12)\n",
        "for _ in range(100):\n",
        "    data.append((random.gauss(5, 0.5), random.gauss(12, 0.5)))\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"x\", \"y\"])\n",
        "\n",
        "print(\"Sample Data:\")\n",
        "df.show(5)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3. Convert Columns into Feature Vector\n",
        "# ---------------------------------------------\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"x\", \"y\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "dataset = assembler.transform(df)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4. Train K-Means Model\n",
        "# ---------------------------------------------\n",
        "kmeans = KMeans() \\\n",
        "    .setK(3) \\\n",
        "    .setSeed(1) \\\n",
        "    .setFeaturesCol(\"features\") \\\n",
        "    .setPredictionCol(\"cluster\")\n",
        "\n",
        "model = kmeans.fit(dataset)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5. Make Predictions\n",
        "# ---------------------------------------------\n",
        "predictions = model.transform(dataset)\n",
        "\n",
        "print(\"Cluster Assignments:\")\n",
        "predictions.select(\"x\", \"y\", \"cluster\").show(10)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 6. Print Cluster Centers\n",
        "# ---------------------------------------------\n",
        "centers = model.clusterCenters()\n",
        "\n",
        "print(\"Cluster Centers:\")\n",
        "for i, center in enumerate(centers):\n",
        "    print(f\"Cluster {i}: {center}\")\n",
        "\n",
        "\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Evaluate using Silhouette Score\n",
        "# ---------------------------------------------\n",
        "evaluator = ClusteringEvaluator(\n",
        "    featuresCol=\"features\",\n",
        "    predictionCol=\"cluster\",\n",
        "    metricName=\"silhouette\",\n",
        "    distanceMeasure=\"squaredEuclidean\"\n",
        ")\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Silhouette Score =\", silhouette)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 7. Stop Spark Session\n",
        "# ---------------------------------------------\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7MF3PAolMAu",
        "outputId": "e398a831-9340-4308-9b93-2a04d58d3c59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Data:\n",
            "+------------------+------------------+\n",
            "|                 x|                 y|\n",
            "+------------------+------------------+\n",
            "| 2.142931171001465|1.7553987054858362|\n",
            "|2.0965233228962723|1.8124620754314489|\n",
            "|3.1611847805974573|1.5474923426841019|\n",
            "|2.4627082038244725|1.5553530309118573|\n",
            "| 2.154144869166751|1.9375302684215185|\n",
            "+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "Cluster Assignments:\n",
            "+------------------+------------------+-------+\n",
            "|                 x|                 y|cluster|\n",
            "+------------------+------------------+-------+\n",
            "| 2.142931171001465|1.7553987054858362|      1|\n",
            "|2.0965233228962723|1.8124620754314489|      1|\n",
            "|3.1611847805974573|1.5474923426841019|      1|\n",
            "|2.4627082038244725|1.5553530309118573|      1|\n",
            "| 2.154144869166751|1.9375302684215185|      1|\n",
            "| 1.752965208665309| 2.473069050715892|      1|\n",
            "|1.7329842153400274| 2.661099857346823|      1|\n",
            "|2.4267711512894157|1.5080098984766086|      1|\n",
            "|1.6729817530368574| 1.077658729676557|      1|\n",
            "|2.3004086654956803| 1.699168585270368|      1|\n",
            "+------------------+------------------+-------+\n",
            "only showing top 10 rows\n",
            "Cluster Centers:\n",
            "Cluster 0: [8.08668112 8.00635588]\n",
            "Cluster 1: [1.88001022 1.99516535]\n",
            "Cluster 2: [ 4.88775278 11.93008873]\n",
            "Silhouette Score = 0.9677591296986323\n"
          ]
        }
      ]
    }
  ]
}