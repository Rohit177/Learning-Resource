### Perform Backup and Restore of Datasets in HDFS

### 1. Load Dataset into HDFS

**Step A: Create a local file**
Create a dummy file.

```bash
echo "Production Data" > prod_data.txt

```

**Step B: Create HDFS directory**
Create a directory to simulate a live production folder.

```bash
hdfs dfs -mkdir -p /user/hadoop/production

```

**Step C: Upload the file**
Upload the local file to the HDFS production folder.

```bash
hdfs dfs -put prod_data.txt /user/hadoop/production/

```

**Step D: Verify Upload**

```bash
hdfs dfs -ls /user/hadoop/production/

```

---

### 2. Copy Dataset to Backup Directory

**Step A: Create Backup Directory**

```bash
hdfs dfs -mkdir -p /user/hadoop/backup

```

**Step B: Perform Backup (Copy)**
Use `cp` to copy the file from production to backup, rename `.bak` extension.

```bash
hdfs dfs -cp /user/hadoop/production/prod_data.txt /user/hadoop/backup/prod_data.txt.bak

```

**Step C: Verify Backup**
Check that the file exists.

```bash
hdfs dfs -ls /user/hadoop/backup/

```

---

### 3. Simulate Accidental Data Removal

**Step A: Delete Original Data**
Remove the file from the production folder.

```bash
hdfs dfs -rm /user/hadoop/production/prod_data.txt

```

**Step B: Verify Deletion**
Confirm the file is gone.

```bash
hdfs dfs -ls /user/hadoop/production/

```

*Output should show an empty directory.*

---

### 4. Restore the Dataset from Backup

**Step A: Restore Data**
Copy the file from `/backup` back to `/production`, renaming it to the original name.

```bash
hdfs dfs -cp /user/hadoop/backup/prod_data.txt.bak /user/hadoop/production/prod_data.txt

```

**Step B: Verify Restoration**
Check if the file is back in the production folder.

```bash
hdfs dfs -ls /user/hadoop/production/

```

**Step C: Check Integrity**
Read the file content to ensure it is correct.

```bash
hdfs dfs -cat /user/hadoop/production/prod_data.txt

```

*Expected Output:* `Important Production Data - Do Not Lose`
