### 1. Configure Hadoop Files
Edit files in `C:\hadoop\etc\hadoop\`.

#### **A. hadoop-env.cmd**
Set the Java path. If installed in "Program Files", use `PROGRA~1` to handle the space.

```cmd
set JAVA_HOME=C:\PROGRA~1\Java\jdk1.8.0_xxx

```

#### **B. core-site.xml**
```xml
<configuration>
   <property>
       <name>fs.defaultFS</name>
       <value>hdfs://localhost:9000</value>
   </property>
</configuration>

```

#### **C. hdfs-site.xml**
Define where data is stored. Create the folders `data/namenode` and `data/datanode` manually in your Hadoop folder first.

```xml
<configuration>
   <property>
       <name>dfs.replication</name>
       <value>1</value>
   </property>
   <property>
       <name>dfs.namenode.name.dir</name>
       <value>/hadoop/data/namenode</value>
   </property>
   <property>
       <name>dfs.datanode.data.dir</name>
       <value>/hadoop/data/datanode</value>
   </property>
</configuration>

```

#### **D. mapred-site.xml**
```xml
<configuration>
   <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
   </property>
</configuration>

```

#### **E. yarn-site.xml**
```xml
<configuration>
   <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
   </property>
</configuration>

```

### 2. Initialization & Execution
Open Command Prompt **as Administrator**.


**1. Format the NameNode:**

```cmd
hdfs namenode -format
# optional if already done
```

**2. Start All Services:**

```cmd
start-all.cmd

```

* This will open 4 separate windows (NameNode, DataNode, ResourceManager, NodeManager).

### 3. Verify
* **HDFS Web UI:** Open browser to `http://localhost:9870`
* **YARN Web UI:** Open browser to `http://localhost:8088`

---

### 4. Upload Dataset to HDFS

#### **A. Create a Local Dataset**

Create a simple text file to test.

```bash
echo "Hello Hadoop World" > test_data.txt

```

#### **B. Create a Directory in HDFS**

Create a folder named `input` inside HDFS.

```bash
hdfs dfs -mkdir -p /user/hadoop/input

```

#### **C. Upload the File**

Copy the local file to the HDFS directory.

```bash
hdfs dfs -put test_data.txt /user/hadoop/input/

```

#### **D. Verify the Upload**

List the files in the HDFS directory to confirm.

```bash
hdfs dfs -ls /user/hadoop/input/

```

**Expected Output:**

```text
Found 1 items
-rw-r--r--   1 hadoop supergroup         19 2025-12-27 12:00 /user/hadoop/input/test_data.txt

```
