{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° Data Transformations Using Spark RDDs\n",
        "\n",
        "Apache **Spark RDDs (Resilient Distributed Datasets)** support powerful transformations such as **map**, **filter**, and **reduce** for large-scale data processing.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Dataset (`numbers.txt`)\n",
        "\n",
        "```text\n",
        "10\n",
        "20\n",
        "30\n",
        "40\n",
        "50\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Step 1: Load Dataset into an RDD\n",
        "\n",
        "Start **Spark Shell** and load the data:\n",
        "\n",
        "```scala\n",
        "val rdd = sc.textFile(\"numbers.txt\").map(_.toInt)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Step 2: Apply `map` Operation\n",
        "\n",
        "Multiply each number by 2:\n",
        "\n",
        "```scala\n",
        "val mappedRDD = rdd.map(x => x * 2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Step 3: Apply `filter` Operation\n",
        "\n",
        "Filter values greater than 50:\n",
        "\n",
        "```scala\n",
        "val filteredRDD = mappedRDD.filter(x => x > 50)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Step 4: Apply `reduce` Operation\n",
        "\n",
        "Find the sum of the filtered values:\n",
        "\n",
        "```scala\n",
        "val result = filteredRDD.reduce((a, b) => a + b)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Output Verification\n",
        "\n",
        "```scala\n",
        "result\n",
        "```\n",
        "\n",
        "```text\n",
        "140\n",
        "```\n"
      ],
      "metadata": {
        "id": "i4fIX5U_E2G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **In PySpark:**"
      ],
      "metadata": {
        "id": "HwO9vPO_xVhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RDD_Data_Transformations\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# -----------------------------------\n",
        "# Load dataset directly into RDD\n",
        "data = [\"Hello Spark\",\n",
        "        \"RDD Transformations\",\n",
        "        \"Spark Map Filter Reduce\",\n",
        "        \"Big Data Processing\"]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "print(\"Original Data:\", rdd.collect())\n",
        "\n",
        "# -----------------------------------\n",
        "# 1Ô∏è‚É£ MAP Operation\n",
        "# Convert each line to lowercase\n",
        "mapped_rdd = rdd.map(lambda line: line.lower())\n",
        "print(\"After map (lowercase):\", mapped_rdd.collect())\n",
        "\n",
        "# -----------------------------------\n",
        "# 2Ô∏è‚É£ FILTER Operation\n",
        "# Keep only lines containing 'spark'\n",
        "filtered_rdd = mapped_rdd.filter(lambda line: \"spark\" in line)\n",
        "print(\"After filter (contains 'spark'):\", filtered_rdd.collect())\n",
        "\n",
        "# -----------------------------------\n",
        "# 3Ô∏è‚É£ REDUCE Operation\n",
        "# Count total number of lines\n",
        "line_count = rdd.map(lambda line: 1).reduce(lambda a, b: a + b)\n",
        "print(\"Total Number of Lines:\", line_count)\n",
        "\n",
        "# Stop Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "QxG6-HgJxTFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}