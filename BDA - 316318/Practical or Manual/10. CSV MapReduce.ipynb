{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fq0_vhJjhtM433TizfhWDPnOMIWW3v2C","timestamp":1768362398925}],"authorship_tag":"ABX9TyMFnJ/q6ZHOU96xyKUiIDBg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Processing a Large CSV Dataset Using MapReduce\n","\n","This guide demonstrates how to:\n","1. Load a CSV dataset into HDFS\n","2. Run a MapReduce job to extract specific fields\n","3. Aggregate data using a Reducer\n","\n","The example uses **Hadoop Streaming with Python**, which is simple and widely supported.\n","\n","---\n","\n","## Prerequisites\n","\n","- Hadoop installed and configured\n","- HDFS running\n","- Python 3 installed on all nodes\n","- Environment variables set:\n","  - `HADOOP_HOME`\n","  - `PATH` includes `$HADOOP_HOME/bin`\n","\n","Verify Hadoop:\n","```bash\n","hadoop version\n","````\n","\n","---\n","\n","## Example Dataset\n","\n","Assume a CSV file named `sales.csv`:\n","\n","\n","### Goal\n","\n","* **Extract**: `region` and `quantity`\n","* **Aggregate**: Total quantity sold per region\n","\n","---\n","\n","## Step 1: Load CSV into HDFS\n","\n","Create a directory in HDFS:\n","\n","```bash\n","hdfs dfs -mkdir -p /data/input\n","```\n","\n","Upload the CSV file:\n","\n","```bash\n","hdfs dfs -put sales.csv /data/input/\n","```\n","\n","Verify upload:\n","\n","```bash\n","hdfs dfs -ls /data/input\n","```\n","\n","---\n","\n","## Step 2: Create the Mapper\n","\n","The mapper:\n","\n","* Skips the header row\n","* Extracts `region` and `quantity`\n","* Outputs key-value pairs: `<region, quantity>`\n","\n","Create `mapper.py`:\n","\n","```python\n","#!/usr/bin/env python3\n","import sys\n","\n","for line in sys.stdin:\n","    line = line.strip()\n","    if line.startswith(\"order_id\"):\n","        continue  # skip header\n","\n","    fields = line.split(\",\")\n","    region = fields[1]\n","    quantity = int(fields[3])\n","\n","    print(f\"{region}\\t{quantity}\")\n","```\n","\n","Make it executable:\n","\n","```bash\n","chmod +x mapper.py\n","```\n","\n","---\n","\n","## Step 3: Create the Reducer\n","\n","The reducer:\n","\n","* Sums quantities per region\n","\n","Create `reducer.py`:\n","\n","```python\n","#!/usr/bin/env python3\n","import sys\n","\n","current_region = None\n","total_quantity = 0\n","\n","for line in sys.stdin:\n","    region, quantity = line.strip().split(\"\\t\")\n","    quantity = int(quantity)\n","\n","    if current_region == region:\n","        total_quantity += quantity\n","    else:\n","        if current_region:\n","            print(f\"{current_region}\\t{total_quantity}\")\n","        current_region = region\n","        total_quantity = quantity\n","\n","if current_region:\n","    print(f\"{current_region}\\t{total_quantity}\")\n","```\n","\n","Make it executable:\n","\n","```bash\n","chmod +x reducer.py\n","```\n","\n","---\n","\n","## Step 4: Run the MapReduce Job\n","\n","Remove output directory if it exists:\n","\n","```bash\n","hdfs dfs -rm -r /data/output\n","```\n","\n","Run Hadoop Streaming:\n","\n","```bash\n","hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n","  -input /data/input/sales.csv \\\n","  -output /data/output \\\n","  -mapper mapper.py \\\n","  -reducer reducer.py \\\n","  -file mapper.py \\\n","  -file reducer.py\n","```\n","\n","---\n","\n","## Step 5: View the Results\n","\n","Display the output:\n","\n","```bash\n","hdfs dfs -cat /data/output/part-00000\n","```\n","\n","### Sample Output\n","\n","```text\n","EU      2\n","US      5\n","```\n","\n","---\n"],"metadata":{"id":"i4fIX5U_E2G5"}}]}